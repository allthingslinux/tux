name: "Docker Performance Testing"

on:
  push:
    branches: ["main", "dev"]
    paths:
      - "Dockerfile"
      - "docker-compose*.yml"
      - ".dockerignore"
      - "pyproject.toml"
      - "poetry.lock"
      - "prisma/schema/**"
      - "scripts/docker-toolkit.sh"
      - ".github/workflows/docker-test.yml"
  pull_request:
    paths:
      - "Dockerfile"
      - "docker-compose*.yml"
      - ".dockerignore"
      - "pyproject.toml"
      - "poetry.lock"
      - "prisma/schema/**"
      - "scripts/docker-toolkit.sh"
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: false
        default: 'test'
        type: choice
        options:
          - quick
          - test
          - comprehensive
  schedule:
    # Run performance tests nightly with comprehensive testing
    - cron: '0 2 * * *'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  # Configurable performance thresholds
  BUILD_THRESHOLD: 300000    # 5 minutes
  STARTUP_THRESHOLD: 10000   # 10 seconds
  PYTHON_THRESHOLD: 5000     # 5 seconds
  MEMORY_THRESHOLD: 512      # 512 MB

jobs:
  # Automatic testing job for push/PR events
  docker-test-auto:
    runs-on: ubuntu-latest
    if: github.event_name != 'workflow_dispatch' && github.event_name != 'schedule'
    permissions:
      contents: read
      packages: read
      issues: write
      pull-requests: write
    
    strategy:
      matrix:
        test-type: [quick, standard]
        include:
          - test-type: quick
            description: "Quick validation (2-3 min)"
            timeout: 10
          - test-type: standard
            description: "Standard testing (5-7 min)"
            timeout: 15
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install performance monitoring tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc time
          
      - name: Create performance tracking directories
        run: |
          mkdir -p logs performance-history artifacts
          
      - name: Set up environment file
        run: |
          # Create minimal .env for testing with all required variables
          cat > .env << EOF
          DEV_DATABASE_URL=sqlite:///tmp/test.db
          PROD_DATABASE_URL=sqlite:///tmp/test.db
          DEV_BOT_TOKEN=test_token_dev_$(date +%s)
          PROD_BOT_TOKEN=test_token_prod_$(date +%s)
          DEV_COG_IGNORE_LIST=rolecount,mail,git
          PROD_COG_IGNORE_LIST=rolecount,mail,git
          DISCORD_TOKEN=test_token_$(date +%s)
          DATABASE_URL=sqlite:///tmp/test.db
          BOT_TOKEN=test_token_$(date +%s)
          COG_IGNORE_LIST=rolecount,mail,git
          SENTRY_DSN=
          LOG_LEVEL=INFO
          EOF
          
          echo "Created .env file with contents:"
          cat .env

      - name: Run ${{ matrix.description }}
        timeout-minutes: ${{ matrix.timeout }}
        run: |
          chmod +x scripts/docker-toolkit.sh
          
          # Enable debug output for CI troubleshooting
          set -x
          
          # Show current environment
          echo "=== Environment Info ==="
          echo "Event name: ${{ github.event_name }}"
          echo "Matrix test-type: ${{ matrix.test-type }}"
          echo "========================"
          
          # Run the test command based on matrix type
          if [ "${{ matrix.test-type }}" = "quick" ]; then
            echo "Running quick validation tests..."
            ./scripts/docker-toolkit.sh quick || {
              echo "Quick tests failed with exit code $?"
              echo "Continuing to collect logs and artifacts..."
              exit 1
            }
          else
            echo "Running standard tests..."
            ./scripts/docker-toolkit.sh test || {
              echo "Standard tests failed with exit code $?"
              echo "Continuing to collect logs and artifacts..."
              exit 1
            }
          fi
          
          echo "Tests completed successfully!"
          
      - name: Collect system performance metrics
        if: always()
        run: |
          echo "System Performance Baseline:" > artifacts/system-info.txt
          echo "============================" >> artifacts/system-info.txt
          echo "Date: $(date -Iseconds)" >> artifacts/system-info.txt
          echo "Runner: ${{ runner.os }}" >> artifacts/system-info.txt
          echo "Architecture: $(uname -m)" >> artifacts/system-info.txt
          echo "Test Type: ${{ matrix.test-type }}" >> artifacts/system-info.txt
          echo "CPU Info:" >> artifacts/system-info.txt
          nproc >> artifacts/system-info.txt
          echo "Memory Info:" >> artifacts/system-info.txt
          free -h >> artifacts/system-info.txt
          echo "Disk Info:" >> artifacts/system-info.txt
          df -h >> artifacts/system-info.txt
          echo "Docker Version:" >> artifacts/system-info.txt
          docker --version >> artifacts/system-info.txt
          echo "Docker Info:" >> artifacts/system-info.txt
          docker system df >> artifacts/system-info.txt

      - name: Analyze build performance
        if: always()
        run: |
          echo "Build Performance Analysis (${{ matrix.test-type }}):" > artifacts/build-analysis.txt
          echo "====================================" >> artifacts/build-analysis.txt
          
          # Extract build metrics from logs (updated for new toolkit)
          if ls logs/docker-*.log 1> /dev/null 2>&1; then
            echo "Build Times:" >> artifacts/build-analysis.txt
            grep -h "completed in\|Build.*:" logs/docker-*.log 2>/dev/null >> artifacts/build-analysis.txt || echo "No build time data found" >> artifacts/build-analysis.txt
            
            echo "" >> artifacts/build-analysis.txt
            echo "Image Sizes:" >> artifacts/build-analysis.txt
            grep -h "image size\|Size:" logs/docker-*.log 2>/dev/null >> artifacts/build-analysis.txt || echo "No image size data found" >> artifacts/build-analysis.txt
            
            echo "" >> artifacts/build-analysis.txt
            echo "Performance Metrics:" >> artifacts/build-analysis.txt
            grep -h "📊\|⚡\|🔧" logs/docker-*.log 2>/dev/null >> artifacts/build-analysis.txt || echo "No performance metrics found" >> artifacts/build-analysis.txt
          else
            echo "No log files found" >> artifacts/build-analysis.txt
          fi

      - name: Check performance thresholds
        if: matrix.test-type == 'standard'
        run: |
          echo "Performance Threshold Check:" > artifacts/threshold-check.txt
          echo "============================" >> artifacts/threshold-check.txt
          
          # Initialize failure flag
          THRESHOLD_FAILED=false
          
          # Check for metrics files from the new toolkit
          if ls logs/docker-metrics-*.json 1> /dev/null 2>&1; then
            metrics_file=$(ls logs/docker-metrics-*.json | head -1)
            echo "Using metrics file: $metrics_file" >> artifacts/threshold-check.txt
            
            if command -v jq &> /dev/null; then
              # Helper function to safely compare numeric values
              safe_compare() {
                local value="$1"
                local threshold="$2"
                local name="$3"
                
                # Check if value is numeric and not null/N/A
                if [[ "$value" =~ ^[0-9]+(\.[0-9]+)?$ ]] && [ "$value" != "null" ] && [ "$value" != "N/A" ]; then
                  # Convert to integer for comparison
                  local int_value=$(printf "%.0f" "$value")
                  if [ "$int_value" -gt "$threshold" ]; then
                    echo "❌ FAIL: $name (${int_value}ms) exceeds threshold (${threshold}ms)" >> artifacts/threshold-check.txt
                    return 1
                  else
                    echo "✅ PASS: $name (${int_value}ms) within threshold (${threshold}ms)" >> artifacts/threshold-check.txt
                    return 0
                  fi
                else
                  echo "⚠️  SKIP: $name value ($value) is not numeric, skipping check" >> artifacts/threshold-check.txt
                  return 0
                fi
              }
              
              # Check build time
              build_time=$(jq -r '.performance.production_build.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              safe_compare "$build_time" "$BUILD_THRESHOLD" "Build time" || THRESHOLD_FAILED=true
              
              # Check startup time
              startup_time=$(jq -r '.performance.container_startup.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              safe_compare "$startup_time" "$STARTUP_THRESHOLD" "Startup time" || THRESHOLD_FAILED=true
              
              # Check Python validation time
              python_time=$(jq -r '.performance.python_validation.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              safe_compare "$python_time" "$PYTHON_THRESHOLD" "Python validation" || THRESHOLD_FAILED=true
              
              # Check image size (with proper rounding)
              image_size_raw=$(jq -r '.performance.prod_image_size_mb.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [[ "$image_size_raw" =~ ^[0-9]+(\.[0-9]+)?$ ]] && [ "$image_size_raw" != "null" ] && [ "$image_size_raw" != "N/A" ]; then
                # Properly round to nearest integer
                image_size=$(printf "%.0f" "$image_size_raw")
                SIZE_THRESHOLD=1024  # 1GB for the new optimized images
                if [ "$image_size" -gt "$SIZE_THRESHOLD" ]; then
                  echo "❌ FAIL: Image size (${image_size}MB) exceeds threshold (${SIZE_THRESHOLD}MB)" >> artifacts/threshold-check.txt
                  THRESHOLD_FAILED=true
                else
                  echo "✅ PASS: Image size (${image_size}MB) within threshold (${SIZE_THRESHOLD}MB)" >> artifacts/threshold-check.txt
                fi
              else
                echo "⚠️  SKIP: Image size value ($image_size_raw) is not numeric, skipping check" >> artifacts/threshold-check.txt
              fi
              
              # Fail the step if any threshold was exceeded
              if [ "$THRESHOLD_FAILED" = true ]; then
                echo ""
                echo "❌ Performance thresholds exceeded!"
                cat artifacts/threshold-check.txt
                exit 1
              else
                echo ""
                echo "✅ All performance thresholds within acceptable ranges"
                cat artifacts/threshold-check.txt
              fi
            else
              echo "jq not available for threshold checking" >> artifacts/threshold-check.txt
            fi
          else
            echo "No metrics files found for threshold checking" >> artifacts/threshold-check.txt
            echo "This may be expected for quick tests" >> artifacts/threshold-check.txt
          fi

      - name: Docker Scout security scan
        if: matrix.test-type == 'standard' && github.event_name != 'pull_request'
        continue-on-error: true
        run: |
          echo "Security Performance Analysis:" > artifacts/security-analysis.txt
          echo "=============================" >> artifacts/security-analysis.txt
          
          # Time the security scan
          start_time=$(date +%s%N)
          
          if docker scout version &> /dev/null; then
            # Use existing test images if available, otherwise build one
            if docker images | grep -q "tux:test-prod"; then
              test_image="tux:test-prod"
            else
              docker build --target production -t tux:security-test . > /dev/null 2>&1
              test_image="tux:security-test"
            fi
            
            # Run security scan
            docker scout cves "$test_image" --only-severity critical,high > artifacts/security-scan.txt 2>&1 || true
            
            # Calculate scan time
            end_time=$(date +%s%N)
            scan_time=$(((end_time - start_time) / 1000000))
            
            echo "Security scan completed in: $scan_time ms" >> artifacts/security-analysis.txt
            echo "SECURITY_SCAN_TIME=$scan_time" >> $GITHUB_ENV
            
            # Count vulnerabilities
            critical_count=$(grep -c "critical" artifacts/security-scan.txt 2>/dev/null || echo "0")
            high_count=$(grep -c "high" artifacts/security-scan.txt 2>/dev/null || echo "0")
            
            echo "Critical vulnerabilities: $critical_count" >> artifacts/security-analysis.txt
            echo "High vulnerabilities: $high_count" >> artifacts/security-analysis.txt
            echo "CRITICAL_VULNS=$critical_count" >> $GITHUB_ENV
            echo "HIGH_VULNS=$high_count" >> $GITHUB_ENV
            
            # Cleanup security test image if we created it
            if [ "$test_image" = "tux:security-test" ]; then
              docker rmi tux:security-test > /dev/null 2>&1 || true
            fi
          else
            echo "Docker Scout not available" >> artifacts/security-analysis.txt
          fi

      - name: Generate performance report
        if: always()
        run: |
          echo "# Docker Performance Report (${{ matrix.test-type }})" > artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Generated:** $(date -Iseconds)" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Commit:** ${{ github.sha }}" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Branch:** ${{ github.ref_name }}" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Test Type:** ${{ matrix.test-type }}" >> artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          
          echo "## Performance Summary" >> artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          
          if ls logs/docker-metrics-*.json 1> /dev/null 2>&1; then
            metrics_file=$(ls logs/docker-metrics-*.json | head -1)
            
            if command -v jq &> /dev/null; then
              echo "| Metric | Value | Status |" >> artifacts/PERFORMANCE-REPORT.md
              echo "|--------|-------|--------|" >> artifacts/PERFORMANCE-REPORT.md
              
              # Production build (if available)
              build_time=$(jq -r '.performance.production_build.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [ "$build_time" != "N/A" ] && [ "$build_time" != "null" ] && [[ "$build_time" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                build_status="✅"
                build_int=$(printf "%.0f" "$build_time")
                [ "$build_int" -gt "$BUILD_THRESHOLD" ] && build_status="❌"
                echo "| Production Build | ${build_time} ms | $build_status |" >> artifacts/PERFORMANCE-REPORT.md
              fi
              
              # Container startup (if available)
              startup_time=$(jq -r '.performance.container_startup.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [ "$startup_time" != "N/A" ] && [ "$startup_time" != "null" ] && [[ "$startup_time" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                startup_status="✅"
                startup_int=$(printf "%.0f" "$startup_time")
                [ "$startup_int" -gt "$STARTUP_THRESHOLD" ] && startup_status="❌"
                echo "| Container Startup | ${startup_time} ms | $startup_status |" >> artifacts/PERFORMANCE-REPORT.md
              fi
              
              # Image size (if available)
              image_size=$(jq -r '.performance.prod_image_size_mb.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [ "$image_size" != "N/A" ] && [ "$image_size" != "null" ] && [[ "$image_size" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                size_status="✅"
                size_int=$(printf "%.0f" "$image_size")
                [ "$size_int" -gt 1024 ] && size_status="❌"
                echo "| Image Size | ${image_size} MB | $size_status |" >> artifacts/PERFORMANCE-REPORT.md
              fi
              
              # Security scan (if available)
              if [ -n "${SECURITY_SCAN_TIME:-}" ]; then
                scan_status="✅"
                [ "${CRITICAL_VULNS:-0}" -gt 0 ] && scan_status="❌"
                echo "| Security Scan | ${SECURITY_SCAN_TIME} ms | $scan_status |" >> artifacts/PERFORMANCE-REPORT.md
                echo "| Critical Vulns | ${CRITICAL_VULNS:-0} | ${scan_status} |" >> artifacts/PERFORMANCE-REPORT.md
              fi
            fi
          else
            echo "No metrics data available for this test type." >> artifacts/PERFORMANCE-REPORT.md
            echo "This is expected for quick validation tests." >> artifacts/PERFORMANCE-REPORT.md
          fi
          
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          echo "## Test Output Summary" >> artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          echo "See attached artifacts for detailed test results and logs." >> artifacts/PERFORMANCE-REPORT.md

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: docker-performance-${{ matrix.test-type }}-${{ github.sha }}
          path: |
            artifacts/
            logs/
          retention-days: 30

      - name: Comment performance results on PR
        if: github.event_name == 'pull_request' && matrix.test-type == 'standard'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## 🔧 Docker Performance Test Results\n\n';
            
            // Read performance report if it exists
            try {
              const report = fs.readFileSync('artifacts/PERFORMANCE-REPORT.md', 'utf8');
              comment += report;
            } catch (e) {
              comment += 'Performance report not generated.\n';
            }
            
            // Add threshold check results
            try {
              const thresholds = fs.readFileSync('artifacts/threshold-check.txt', 'utf8');
              comment += '\n## Threshold Checks\n\n```\n' + thresholds + '\n```\n';
            } catch (e) {
              comment += '\nThreshold check results not available.\n';
            }
            
            comment += '\n📊 [View detailed performance data](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
  
  # Manual testing job for workflow_dispatch events
  docker-test-manual:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    permissions:
      contents: read
      packages: read
      issues: write
      pull-requests: write
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install performance monitoring tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc time
          
      - name: Create performance tracking directories
        run: |
          mkdir -p logs performance-history artifacts
          
      - name: Set up environment file
        run: |
          # Create minimal .env for testing with all required variables
          cat > .env << EOF
          DEV_DATABASE_URL=sqlite:///tmp/test.db
          PROD_DATABASE_URL=sqlite:///tmp/test.db
          DEV_BOT_TOKEN=test_token_dev_$(date +%s)
          PROD_BOT_TOKEN=test_token_prod_$(date +%s)
          DEV_COG_IGNORE_LIST=rolecount,mail,git
          PROD_COG_IGNORE_LIST=rolecount,mail,git
          DISCORD_TOKEN=test_token_$(date +%s)
          DATABASE_URL=sqlite:///tmp/test.db
          BOT_TOKEN=test_token_$(date +%s)
          COG_IGNORE_LIST=rolecount,mail,git
          SENTRY_DSN=
          LOG_LEVEL=INFO
          EOF
          
          echo "Created .env file with contents:"
          cat .env

      - name: Run manual test (${{ github.event.inputs.test_level }})
        timeout-minutes: 20
        run: |
          chmod +x scripts/docker-toolkit.sh
          
          # Enable debug output for CI troubleshooting
          set -x
          
          # Show current environment
          echo "=== Environment Info ==="
          echo "Event name: ${{ github.event_name }}"
          echo "Test level input: ${{ github.event.inputs.test_level }}"
          echo "========================"
          
          # Run the test command based on input
          test_level="${{ github.event.inputs.test_level }}"
          case "$test_level" in
            "quick")
              echo "Running quick validation tests..."
              ./scripts/docker-toolkit.sh quick || {
                echo "Quick tests failed with exit code $?"
                echo "Continuing to collect logs and artifacts..."
                exit 1
              }
              ;;
            "comprehensive")
              echo "Running comprehensive tests..."
              ./scripts/docker-toolkit.sh comprehensive || {
                echo "Comprehensive tests failed with exit code $?"
                echo "Continuing to collect logs and artifacts..."
                exit 1
              }
              ;;
            *)
              echo "Running standard tests..."
              ./scripts/docker-toolkit.sh test || {
                echo "Standard tests failed with exit code $?"
                echo "Continuing to collect logs and artifacts..."
                exit 1
              }
              ;;
          esac
          
          echo "Tests completed successfully!"
          
      - name: Collect system performance metrics
        if: always()
        run: |
          echo "System Performance Baseline:" > artifacts/system-info.txt
          echo "============================" >> artifacts/system-info.txt
          echo "Date: $(date -Iseconds)" >> artifacts/system-info.txt
          echo "Runner: ${{ runner.os }}" >> artifacts/system-info.txt
          echo "Architecture: $(uname -m)" >> artifacts/system-info.txt
          echo "Test Type: ${{ github.event.inputs.test_level }}" >> artifacts/system-info.txt
          echo "CPU Info:" >> artifacts/system-info.txt
          nproc >> artifacts/system-info.txt
          echo "Memory Info:" >> artifacts/system-info.txt
          free -h >> artifacts/system-info.txt
          echo "Disk Info:" >> artifacts/system-info.txt
          df -h >> artifacts/system-info.txt
          echo "Docker Version:" >> artifacts/system-info.txt
          docker --version >> artifacts/system-info.txt
          echo "Docker Info:" >> artifacts/system-info.txt
          docker system df >> artifacts/system-info.txt

      - name: Analyze build performance
        if: always()
        run: |
          echo "Build Performance Analysis (${{ github.event.inputs.test_level }}):" > artifacts/build-analysis.txt
          echo "====================================" >> artifacts/build-analysis.txt
          
          # Extract build metrics from logs (updated for new toolkit)
          if ls logs/docker-*.log 1> /dev/null 2>&1; then
            echo "Build Times:" >> artifacts/build-analysis.txt
            grep -h "completed in\|Build.*:" logs/docker-*.log 2>/dev/null >> artifacts/build-analysis.txt || echo "No build time data found" >> artifacts/build-analysis.txt
            
            echo "" >> artifacts/build-analysis.txt
            echo "Image Sizes:" >> artifacts/build-analysis.txt
            grep -h "image size\|Size:" logs/docker-*.log 2>/dev/null >> artifacts/build-analysis.txt || echo "No image size data found" >> artifacts/build-analysis.txt
            
            echo "" >> artifacts/build-analysis.txt
            echo "Performance Metrics:" >> artifacts/build-analysis.txt
            grep -h "📊\|⚡\|🔧" logs/docker-*.log 2>/dev/null >> artifacts/build-analysis.txt || echo "No performance metrics found" >> artifacts/build-analysis.txt
          else
            echo "No log files found" >> artifacts/build-analysis.txt
          fi

      - name: Check performance thresholds
        if: github.event.inputs.test_level == 'test' || github.event.inputs.test_level == 'comprehensive'
        run: |
          echo "Performance Threshold Check:" > artifacts/threshold-check.txt
          echo "============================" >> artifacts/threshold-check.txt
          
          # Initialize failure flag
          THRESHOLD_FAILED=false
          
          # Check for metrics files from the new toolkit
          if ls logs/docker-metrics-*.json 1> /dev/null 2>&1; then
            metrics_file=$(ls logs/docker-metrics-*.json | head -1)
            echo "Using metrics file: $metrics_file" >> artifacts/threshold-check.txt
            
            if command -v jq &> /dev/null; then
              # Helper function to safely compare numeric values
              safe_compare() {
                local value="$1"
                local threshold="$2"
                local name="$3"
                
                # Check if value is numeric and not null/N/A
                if [[ "$value" =~ ^[0-9]+(\.[0-9]+)?$ ]] && [ "$value" != "null" ] && [ "$value" != "N/A" ]; then
                  # Convert to integer for comparison
                  local int_value=$(printf "%.0f" "$value")
                  if [ "$int_value" -gt "$threshold" ]; then
                    echo "❌ FAIL: $name (${int_value}ms) exceeds threshold (${threshold}ms)" >> artifacts/threshold-check.txt
                    return 1
                  else
                    echo "✅ PASS: $name (${int_value}ms) within threshold (${threshold}ms)" >> artifacts/threshold-check.txt
                    return 0
                  fi
                else
                  echo "⚠️  SKIP: $name value ($value) is not numeric, skipping check" >> artifacts/threshold-check.txt
                  return 0
                fi
              }
              
              # Check build time
              build_time=$(jq -r '.performance.production_build.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              safe_compare "$build_time" "$BUILD_THRESHOLD" "Build time" || THRESHOLD_FAILED=true
              
              # Check startup time
              startup_time=$(jq -r '.performance.container_startup.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              safe_compare "$startup_time" "$STARTUP_THRESHOLD" "Startup time" || THRESHOLD_FAILED=true
              
              # Check Python validation time
              python_time=$(jq -r '.performance.python_validation.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              safe_compare "$python_time" "$PYTHON_THRESHOLD" "Python validation" || THRESHOLD_FAILED=true
              
              # Check image size (with proper rounding)
              image_size_raw=$(jq -r '.performance.prod_image_size_mb.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [[ "$image_size_raw" =~ ^[0-9]+(\.[0-9]+)?$ ]] && [ "$image_size_raw" != "null" ] && [ "$image_size_raw" != "N/A" ]; then
                # Properly round to nearest integer
                image_size=$(printf "%.0f" "$image_size_raw")
                SIZE_THRESHOLD=1024  # 1GB for the new optimized images
                if [ "$image_size" -gt "$SIZE_THRESHOLD" ]; then
                  echo "❌ FAIL: Image size (${image_size}MB) exceeds threshold (${SIZE_THRESHOLD}MB)" >> artifacts/threshold-check.txt
                  THRESHOLD_FAILED=true
                else
                  echo "✅ PASS: Image size (${image_size}MB) within threshold (${SIZE_THRESHOLD}MB)" >> artifacts/threshold-check.txt
                fi
              else
                echo "⚠️  SKIP: Image size value ($image_size_raw) is not numeric, skipping check" >> artifacts/threshold-check.txt
              fi
              
              # Fail the step if any threshold was exceeded
              if [ "$THRESHOLD_FAILED" = true ]; then
                echo ""
                echo "❌ Performance thresholds exceeded!"
                cat artifacts/threshold-check.txt
                exit 1
              else
                echo ""
                echo "✅ All performance thresholds within acceptable ranges"
                cat artifacts/threshold-check.txt
              fi
            else
              echo "jq not available for threshold checking" >> artifacts/threshold-check.txt
            fi
          else
            echo "No metrics files found for threshold checking" >> artifacts/threshold-check.txt
            echo "This may be expected for quick tests" >> artifacts/threshold-check.txt
          fi

      - name: Docker Scout security scan
        if: (github.event.inputs.test_level == 'test' || github.event.inputs.test_level == 'comprehensive') && github.event_name != 'pull_request'
        continue-on-error: true
        run: |
          echo "Security Performance Analysis:" > artifacts/security-analysis.txt
          echo "=============================" >> artifacts/security-analysis.txt
          
          # Time the security scan
          start_time=$(date +%s%N)
          
          if docker scout version &> /dev/null; then
            # Use existing test images if available, otherwise build one
            if docker images | grep -q "tux:test-prod"; then
              test_image="tux:test-prod"
            else
              docker build --target production -t tux:security-test . > /dev/null 2>&1
              test_image="tux:security-test"
            fi
            
            # Run security scan
            docker scout cves "$test_image" --only-severity critical,high > artifacts/security-scan.txt 2>&1 || true
            
            # Calculate scan time
            end_time=$(date +%s%N)
            scan_time=$(((end_time - start_time) / 1000000))
            
            echo "Security scan completed in: $scan_time ms" >> artifacts/security-analysis.txt
            echo "SECURITY_SCAN_TIME=$scan_time" >> $GITHUB_ENV
            
            # Count vulnerabilities
            critical_count=$(grep -c "critical" artifacts/security-scan.txt 2>/dev/null || echo "0")
            high_count=$(grep -c "high" artifacts/security-scan.txt 2>/dev/null || echo "0")
            
            echo "Critical vulnerabilities: $critical_count" >> artifacts/security-analysis.txt
            echo "High vulnerabilities: $high_count" >> artifacts/security-analysis.txt
            echo "CRITICAL_VULNS=$critical_count" >> $GITHUB_ENV
            echo "HIGH_VULNS=$high_count" >> $GITHUB_ENV
            
            # Cleanup security test image if we created it
            if [ "$test_image" = "tux:security-test" ]; then
              docker rmi tux:security-test > /dev/null 2>&1 || true
            fi
          else
            echo "Docker Scout not available" >> artifacts/security-analysis.txt
          fi

      - name: Generate performance report
        if: always()
        run: |
          echo "# Docker Performance Report (${{ github.event.inputs.test_level }})" > artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Generated:** $(date -Iseconds)" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Commit:** ${{ github.sha }}" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Branch:** ${{ github.ref_name }}" >> artifacts/PERFORMANCE-REPORT.md
          echo "**Test Type:** ${{ github.event.inputs.test_level }}" >> artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          
          echo "## Performance Summary" >> artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          
          if ls logs/docker-metrics-*.json 1> /dev/null 2>&1; then
            metrics_file=$(ls logs/docker-metrics-*.json | head -1)
            
            if command -v jq &> /dev/null; then
              echo "| Metric | Value | Status |" >> artifacts/PERFORMANCE-REPORT.md
              echo "|--------|-------|--------|" >> artifacts/PERFORMANCE-REPORT.md
              
              # Production build (if available)
              build_time=$(jq -r '.performance.production_build.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [ "$build_time" != "N/A" ] && [ "$build_time" != "null" ] && [[ "$build_time" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                build_status="✅"
                build_int=$(printf "%.0f" "$build_time")
                [ "$build_int" -gt "$BUILD_THRESHOLD" ] && build_status="❌"
                echo "| Production Build | ${build_time} ms | $build_status |" >> artifacts/PERFORMANCE-REPORT.md
              fi
              
              # Container startup (if available)
              startup_time=$(jq -r '.performance.container_startup.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [ "$startup_time" != "N/A" ] && [ "$startup_time" != "null" ] && [[ "$startup_time" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                startup_status="✅"
                startup_int=$(printf "%.0f" "$startup_time")
                [ "$startup_int" -gt "$STARTUP_THRESHOLD" ] && startup_status="❌"
                echo "| Container Startup | ${startup_time} ms | $startup_status |" >> artifacts/PERFORMANCE-REPORT.md
              fi
              
              # Image size (if available)
              image_size=$(jq -r '.performance.prod_image_size_mb.value // "N/A"' "$metrics_file" 2>/dev/null || echo "N/A")
              if [ "$image_size" != "N/A" ] && [ "$image_size" != "null" ] && [[ "$image_size" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
                size_status="✅"
                size_int=$(printf "%.0f" "$image_size")
                [ "$size_int" -gt 1024 ] && size_status="❌"
                echo "| Image Size | ${image_size} MB | $size_status |" >> artifacts/PERFORMANCE-REPORT.md
              fi
              
              # Security scan (if available)
              if [ -n "${SECURITY_SCAN_TIME:-}" ]; then
                scan_status="✅"
                [ "${CRITICAL_VULNS:-0}" -gt 0 ] && scan_status="❌"
                echo "| Security Scan | ${SECURITY_SCAN_TIME} ms | $scan_status |" >> artifacts/PERFORMANCE-REPORT.md
                echo "| Critical Vulns | ${CRITICAL_VULNS:-0} | ${scan_status} |" >> artifacts/PERFORMANCE-REPORT.md
              fi
            fi
          else
            echo "No metrics data available for this test type." >> artifacts/PERFORMANCE-REPORT.md
            echo "This is expected for quick validation tests." >> artifacts/PERFORMANCE-REPORT.md
          fi
          
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          echo "## Test Output Summary" >> artifacts/PERFORMANCE-REPORT.md
          echo "" >> artifacts/PERFORMANCE-REPORT.md
          echo "See attached artifacts for detailed test results and logs." >> artifacts/PERFORMANCE-REPORT.md

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: docker-performance-manual-${{ github.event.inputs.test_level }}-${{ github.sha }}
          path: |
            artifacts/
            logs/
          retention-days: 30

      - name: Comment performance results on PR
        if: false
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## 🔧 Docker Performance Test Results\n\n';
            
            // Read performance report if it exists
            try {
              const report = fs.readFileSync('artifacts/PERFORMANCE-REPORT.md', 'utf8');
              comment += report;
            } catch (e) {
              comment += 'Performance report not generated.\n';
            }
            
            // Add threshold check results
            try {
              const thresholds = fs.readFileSync('artifacts/threshold-check.txt', 'utf8');
              comment += '\n## Threshold Checks\n\n```\n' + thresholds + '\n```\n';
            } catch (e) {
              comment += '\nThreshold check results not available.\n';
            }
            
            comment += '\n📊 [View detailed performance data](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Comprehensive testing job for scheduled runs and manual triggers
  comprehensive-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_level == 'comprehensive'
    permissions:
      contents: read
      packages: read
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install performance monitoring tools
        run: |
          sudo apt-get update
          sudo apt-get install -y jq bc time

      - name: Set up environment file
        run: |
          # Create minimal .env for testing with all required variables
          cat > .env << EOF
          DEV_DATABASE_URL=sqlite:///tmp/test.db
          PROD_DATABASE_URL=sqlite:///tmp/test.db
          DEV_BOT_TOKEN=test_token_dev_$(date +%s)
          PROD_BOT_TOKEN=test_token_prod_$(date +%s)
          DEV_COG_IGNORE_LIST=rolecount,mail,git
          PROD_COG_IGNORE_LIST=rolecount,mail,git
          DISCORD_TOKEN=test_token_$(date +%s)
          DATABASE_URL=sqlite:///tmp/test.db
          BOT_TOKEN=test_token_$(date +%s)
          COG_IGNORE_LIST=rolecount,mail,git
          SENTRY_DSN=
          LOG_LEVEL=INFO
          EOF

      - name: Run comprehensive Docker testing
        timeout-minutes: 25
        run: |
          chmod +x scripts/docker-toolkit.sh
          ./scripts/docker-toolkit.sh comprehensive

      - name: Upload comprehensive test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: docker-comprehensive-${{ github.sha }}
          path: |
            logs/
          retention-days: 90

  cleanup:
    runs-on: ubuntu-latest
    needs: [docker-test-auto, docker-test-manual, comprehensive-test]
    if: always()
    permissions:
      contents: read
    steps:
      - name: Clean up Docker resources (SAFE - test images only)
        run: |
          # Remove ONLY test images created during this job (safe patterns)
          docker images --format "{{.Repository}}:{{.Tag}}" 2>/dev/null | grep -E "^tux:(test-|quick-|security-|fresh-|cached-|switch-test-|regression-)" | xargs -r docker rmi -f 2>/dev/null || true
          
          # Remove ONLY dangling images (safe)
          docker images --filter "dangling=true" -q 2>/dev/null | xargs -r docker rmi -f 2>/dev/null || true
          
          # Prune ONLY build cache (safe)
          docker builder prune -f 2>/dev/null || true
          
          echo "✅ SAFE cleanup completed - system images preserved" 